{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0910199/0910199/blob/main/Netflix_Movies_And_TV_Shows_Clustering_(Unsupervised_ML).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Netflix Movies and TV Shows Clustering\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Member Name -** Akshay Bansala"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Initially i have to start with understanding the dataset, then i will clean the data to make analysis ready.\n",
        "\n",
        "Explore the data and understand the behaviour of the same.\n",
        "\n",
        "Then i have prepare the dataset for creating clusters by various parameters wherein i can remove stop words, white spaces numbers etc. so that i can get important words and based on that i shall form clusters.\n",
        "\n",
        "Later i have used the silhouette method and k-means elbow method to find optimal number of clusters and built recommender system by cosine similarity and recommended top ten movies."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n",
        "\n",
        "**In this project, required to do:**\n",
        "\n",
        "\n",
        "*   Exploratory Data Analysis.\n",
        "*   Understanding what type content is available in different countries.\n",
        "\n",
        "*   Is Netflix has increasingly focusing on TV rather than movies in recent years.\n",
        "*   Clustering similar content by matching text-based features."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Importing Numpy & Pandas for data processing & data wrangling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Importing  tools for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importing libraries for hypothesis testing\n",
        "from scipy.stats import uniform\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import chi2\n",
        "from scipy.stats import t\n",
        "from scipy.stats import f\n",
        "from scipy.stats import ttest_ind\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Word Cloud library\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Library used for textual data preprocessing\n",
        "import string\n",
        "string.punctuation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# Library used for Clusters implementation\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "# Library used for building recommendation system\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import *\n",
        "\n",
        "# Library used for ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "rWvIFLrSTkzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# View top 5 rows of the dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "# Checking number of rows and columns of the dataset using shape\n",
        "print(\"Number of rows are: \",df.shape[0])\n",
        "print(\"Number of columns are: \",df.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "# Checking information about the dataset using info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "dup = df.duplicated().sum()\n",
        "print(f'number of duplicated rows are {dup}')"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Checking Null Value by Plotting Heatmap\n",
        "sns.heatmap(df.isnull(), cbar=True)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Netflix dataset consists of tv shows and movies available on Netflix as of 2019.\n",
        "* There are 7787 rows and 12 columns provided in the data.\n",
        "* Null values are present in director, cast, country, date_added, and rating; Since there are only few null values present in date_added and rating (10 & 7 respectively) we will remove them from the data.\n",
        "* No duplicate values exist."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe (all columns included)\n",
        "df.describe(include= 'all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> The dataset contains movies and tv shows information (show id, type, title, director, release year, rating, duration etc.).</b>\n",
        "\n",
        "\n",
        "<b>Attribute Information : </b>\n",
        "\n",
        "- **show_id**: Unique Id number for all the listed rows\n",
        "\n",
        "- **type**: denotes type of show namely TV Show or Movie\n",
        "\n",
        "- **title**: title of the movie\n",
        "\n",
        "- **director**: Name of director/directors\n",
        "\n",
        "- **cast**: lists the cast of the movie\n",
        "\n",
        "- **country**: country of the production house\n",
        "\n",
        "- **date_added**: the date the show was added\n",
        "\n",
        "- **release_year**: year of the release of the show\n",
        "\n",
        "- **rating**: show ratings\n",
        "\n",
        "- **duration**: duration of the show\n",
        "\n",
        "- **listed_in**: the genre of the show\n",
        "\n",
        "- **description**: summary/ description of the movie\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable using a for loop.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in\",i,\"is\",df[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before doing any data wrangling lets create copy of the dataset\n",
        "data = df.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling cast null values as not available\n",
        "data['cast'] = data['cast'].fillna(value='Not available')"
      ],
      "metadata": {
        "id": "AA9hCE_uZFBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling country null values as not known, the same can be replaced by mode of the country using the below commented code\n",
        "# Note: only one line should be selected to run this\n",
        "\n",
        "data['country'] = data['country'].fillna(value='Not Known')\n",
        "# data['country'] = data['country'].fillna(value=data['country'].mode())"
      ],
      "metadata": {
        "id": "tfyG4I1kZVeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since date_added and rating have low number of missing values, that are 10 and 7 respectively, i have dropping the same\n",
        "data = data.dropna(subset=['date_added','rating'])"
      ],
      "metadata": {
        "id": "cEDg_DXQZ_6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since director have many null values if we drop them, we loss a lot data so, i replacing them with unknown.\n",
        "data['director'] = data['director'].fillna(value='Unknown')"
      ],
      "metadata": {
        "id": "oxVLHCf6aaK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking missing values again for confirmation\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "iRilsTFmak1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the data analysis ready i have done the following:\n",
        "1. Filled missing values of cast with Not available.\n",
        "2. Filled missing values of country with Not Known.\n",
        "3. Filled missing values of director with Unknown.\n",
        "4. Dropped rows of date_added missing values.\n",
        "5. Dropped rows of ratings missing values."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 : Movies vs TV Shows Share"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 Pie Chart Visualization Code For Movie vs TV Show Share\n",
        "spread = data['type'].value_counts()\n",
        "plt.rcParams['figure.figsize'] = (5,5)\n",
        "\n",
        "# Set Labels\n",
        "spread.plot(kind = 'pie', autopct='%1.2f%%', cmap='Set1')\n",
        "plt.title(f'Movie vs TV Show share')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Wherever different percentage comparison comes into action, pie chart is used frequently. So, i have used Pie Chart and which helped us to get the percentage comparison more clearly and precisely.\n",
        "\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart, we got to know that the types of shows available in netflix is not even with high count for TV shows. 69.14% of the data belongs to movies and 30.86% of the data for TV shows."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the chart can potentially create a positive business impact by providing valuable information for decision-making. Understanding the distribution of categories in various columns helps identify patterns and target specific demographics or areas of focus. For example, businesses can develop tailored marketing campaigns based on the types of shows most watched by the audience."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 : Top 10 Countries with Most Content"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Chart - 2 Count Plot Visualization Code for Content Produced by Different Countries\n",
        "# Not Taking Unknown Countries\n",
        "country_df = data[data['country'] != 'Not Known']\n",
        "\n",
        "# Set Labels\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(y='country', hue='type', data = country_df, palette=['#564d4d', '#db0000'], order=country_df.country.value_counts().iloc[:10].index)\n",
        "plt.title('Top Ten Countries With Most Content')\n",
        "plt.ylabel('Country')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()\n",
        "\n",
        "# Printing The Counts of Different Shows for Top 10 Countries\n",
        "print('Number of Shows Produced by Top 10 Countries:')\n",
        "print(country_df.groupby(['type']).country.value_counts().groupby(level=0, group_keys=False).head(10))"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above count plot we found that the content belongs to United States alone is 2546 (Movie: 1847, TV Show: 699) and followed by India is 923 (Movie: 852, TV Show: 71)."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, from above insight we got to know:\n",
        "* The United States is a leading producer of both types of shows (Movies and TV Shows), this makes sense since Netflix is a US company.\n",
        "\n",
        "* The influence of Bollywood in India explains the type of content available, and perhaps the main focus of this industry is Movies and not TV Shows.\n",
        "\n",
        "* On the other hand, TV Shows are more frequent in South Korea, which explains the KDrama culture nowadays.\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 : Distribution of Various Ratings"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 Count Plot Visualization Code for Various Ratings of Shows\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(x='rating', hue='type', data=data, palette=['#564d4d', '#db0000'])\n",
        "\n",
        "# Set Labels\n",
        "plt.title('Counts of Various Ratings')\n",
        "plt.xlabel('Ratings')\n",
        "plt.xticks(rotation = 60)\n",
        "\n",
        "# Display Chart\n",
        "plt.show()\n",
        "\n",
        "# Printing The Counts of Each Rating for Different Type Shows\n",
        "print('Each Rating Counts for Different Types of Shows:')\n",
        "print(data.groupby(['rating', 'type']).size())"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above count plot we can clearly see that the most of the ratings are given by TV-MA followed by TV-14 and the least ratings are given by NC-17.\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each TV show and movie on Netflix is assigned a maturity rating to help members make informed choices for themselves and their children. Netflix determines maturity ratings by the frequency and impact of mature content in a TV show or movie. TV show ratings reflect the overall maturity level of the whole series.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 : Content Released Over The Years"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create New DataFrames for Movie and TV Show Release\n",
        "release_year_df = data[['type','release_year']]\n",
        "movie_year = release_year_df[release_year_df['type']=='Movie'].release_year.value_counts().to_frame().reset_index()\n",
        "\n",
        "show_year = release_year_df[release_year_df['type']=='TV Show'].release_year.value_counts().to_frame().reset_index()"
      ],
      "metadata": {
        "id": "nyTOdvbAiswk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(movie_year)"
      ],
      "metadata": {
        "id": "Lzt9dyoDTALx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 Line Plot Visualization Code for Content Released Over The Years\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "sns.lineplot(data=movie_year, x='release_year',  y='count', color = '#db0000')\n",
        "sns.lineplot(data=show_year, x='release_year',  y='count', color = '#564d4d')\n",
        "\n",
        "# Set Labels\n",
        "plt.title('Content Released Over The Years')\n",
        "plt.legend(['Movie','TV Show'])\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot, also known as a line chart or line graph, is a way to visualize the trend of a single variable over time. It uses a series of data points connected by a line to show how the value of the variable changes over time.\n",
        "\n",
        "Line plots are useful because they can quickly and easily show trends and patterns in the data. They are particularly useful for showing how a variable changes over a period of time. They are also useful for comparing the trends of multiple variables.\n",
        "\n",
        "To see how the different contents are released over the years i have used line plot here.\n",
        "\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that most of the content on netflix are of the release date from 2010 to 2020."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "* Growth in the number of movies on Netflix is much higher than tv shows.\n",
        "* Most of the content available was released between 2010 and 2020.\n",
        "* The highest number of movies got released in 2017 and 2018 and tv shows got released in 2019 and 2020.\n",
        "* The line plot shows very few movies, and tv shows got released before the year 2010 and in 2021. It is due to very little data collected from the year 2021."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 : Content Added Over The Months"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Month from date_added Column\n",
        "data['month_added'] = pd.DatetimeIndex(data['date_added']).month"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame To Store Month Values and Counts\n",
        "months_df = data.month_added.value_counts().to_frame().reset_index()"
      ],
      "metadata": {
        "id": "XhCelsRXl0x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(months_df)"
      ],
      "metadata": {
        "id": "gwQ_Zc-5XctV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 Count Plot Visualization Code for Month Wise Addition of Contents on Netflix\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax=sns.barplot(data=months_df, x='month_added', y='count', palette='Reds_r')\n",
        "\n",
        "# Set Labels\n",
        "plt.title('Month Wise Addition of Contents')\n",
        "plt.xlabel('Month')\n",
        "for i in ax.patches:\n",
        "    ax.annotate(f'{i.get_height()}', (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 6), textcoords = 'offset points')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hwWa7wNYl-yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that most of the shows are uploaded either by year ending or beginning."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "* October, November, December, and January are months in which many tv shows and movies get uploaded to the platform.\n",
        "* It might be due to the winter, as in these months people may stay at home and watch tv shows and movies in their free time."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 : Duration Distribution for Netflix Movies"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting Movie and Separating Values\n",
        "df_movies = data[data['type']=='Movie'].copy()\n",
        "df_movies.duration = df_movies.duration.str.replace(' min','').astype(int)"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 Histogram Visualization Code for Duration Distribution of Netflix Movies\n",
        "plt.figure(figsize=(8,4), dpi=120)\n",
        "sns.set(style=\"darkgrid\")\n",
        "sns.histplot(df_movies.duration, color='#db0000')\n",
        "plt.xticks(np.arange(0,360,30))\n",
        "\n",
        "# Set Labels\n",
        "plt.title(\"Duration Distribution for Netflix Movies\")\n",
        "plt.ylabel(\"% of All Netflix Movies\", fontsize=9)\n",
        "plt.xlabel(\"Duration (minutes)\", fontsize=9)\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RQ9dOUyEmloH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histplot is a type of chart that displays the distribution of a dataset. It is a graphical representation of the data that shows how often each value or group of values occurs. Histplots are useful for understanding the distribution of a dataset and identifying patterns or trends in the data. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data.\n",
        "\n",
        "Thus, I used the histogram plot to analysis the duration distributions for the netflix movies."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart we come to know that most of the movies last for 90 to 120 minutes.\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "\n",
        "*  On netflix most of the movies last for 90 to 120 minutes.\n",
        "*  So for target audience, movies duration will be greater than minimum 90 minutes."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 : Most Used Words in Shows Title"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 WordCloud Plot Visualization Code for Most Used Words in Netflix Shows Title\n",
        "# Create a String to Store All The Words\n",
        "comment_words = ''\n",
        "\n",
        "# Remove The Stopwords\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "# Iterate Through The Column\n",
        "for val in data.title:\n",
        "\n",
        "    # Typecaste Each Val to String\n",
        "    val = str(val)\n",
        "\n",
        "    # Split The Value\n",
        "    tokens = val.split()\n",
        "\n",
        "    # Converts Each Token into lowercase\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = tokens[i].lower()\n",
        "\n",
        "    comment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "# Set Parameters\n",
        "wordcloud = WordCloud(width = 1000, height = 500,\n",
        "                background_color ='white',\n",
        "                stopwords = stopwords,\n",
        "                min_font_size = 10,\n",
        "                max_words = 1000,\n",
        "                colormap = 'gist_heat_r').generate(comment_words)\n",
        "\n",
        "# Set Labels\n",
        "plt.figure(figsize = (6,6), facecolor = None)\n",
        "plt.title('Most Used Words In Shows Title', fontsize = 15, pad=20)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word cloud graphic is a visual representation that supplements a section of text to help readers better understand an idea or approach a subject from a different angle. A word cloud shows off trends."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above word cloud plot, it is observed that most repeated words in title include Christmas, Love, World, Man, and Story."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "* Most repeated words in title include Christmas, Love, World, Man, and Story.\n",
        "* We saw that most of the movies and tv shows got added during the winters, which tells why Christmas appeared many times in the titles."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 Count Plot Visualization Code for Top 10 Genres on Netflix\n",
        "# Separating Genres\n",
        "genres = df.set_index('title').listed_in.str.split(', ', expand=True).stack()\n",
        "\n",
        "# Set Labels and Ploting Graph for Top 10 Genres\n",
        "plt.figure(figsize=(10,5))\n",
        "g = sns.countplot(y = genres, order=genres.value_counts().index[:10], palette = \"Reds_r\")\n",
        "plt.title('Top 10 Genres on Netflix')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CZsFId9aqvMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that international movies is in top in terms of genre and followed by dramas and comedies.\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "* In terms of genres, international movies takes the cake surprisingly followed by dramas and comedies.\n",
        "* Even though the United States has the most content available, it looks like Netflix has decided to release a ton of international movies."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 : Top 10 Directors on Netflix"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 Count Plot Visualization Code for Top 10 Directors on Netflix\n",
        "directors = data[data.director != 'Unknown'].set_index('title').director.str.split(', ', expand=True).stack()\n",
        "\n",
        "# Set Labels and Ploting Graph for Top 10 Directors\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(y = directors, order=directors.value_counts().index[:10], palette='Reds_r')\n",
        "plt.title('Top 10 Directors on Netflix')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "juHBp6Zhqad9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "From the above chart we come to know that the most popular director in netflix is Jan Sutar and followed by Raúl Campos and Marcus Raboy."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "* Jan Suter, Raúl Campos, Marcus Raboy, Jay Karas, Cathy Garcia-Molina, Jay Chapman are the top 5 directors which highest number of movies and tv shows are available in netflix.\n",
        "* As we stated previously regarding the top genres, it's no surprise that the most popular directors on Netflix with the most titles are mainly international as well."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 : Top 10 Actors on Netflix"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 Count Plot Visualization Code for Top 10 Actors on Netflix\n",
        "actor = data[data.cast != 'Not available'].set_index('title').cast.str.split(', ', expand=True).stack()\n",
        "\n",
        "# Set Labels and Ploting Graph for Top 10 Actors\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(y = actor, order=actor.value_counts().index[:10], palette='Reds_r')\n",
        "plt.title('Top 10 Actors on Netflix')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that most popular actors with most content in netflix are Anupam Kher, Shah Rukh Khan, Naseeruddin Shah and followed by Om Puri and Takahiro Sakurai.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "*  That the actors in the top ten list of most numbers tv shows and movies are from India.\n",
        "*  Anupam Kher and Shah Rukh Khan have 30 above content alone in netflix."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 : Pair Plot"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot Visualization Code\n",
        "sns.pairplot(data, diag_kind=\"kde\", kind = 'reg')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot, also known as a scatterplot matrix, is a visualization that allows you to visualize the relationships between all pairs of variables in a dataset. It is a useful tool for data exploration because it allows you to quickly see how all of the variables in a dataset are related to one another.\n",
        "\n",
        "Thus, we used pair plot to analyse the patterns of data and relationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there is only one value in dataframe of integer type, we are unable to visualize the pair plot.\n",
        "\n"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on above chart experiments i have noticed that some variable of our netflix dataset does not seems to normally distributed so i have made hypothetical assumption that our data is normally distributed and for that i have decided to do statistical analysis.\n",
        "\n",
        "1.   Average number of movies on Netflix in **United States** is greater than the average number of movies on Netflix in **India**.\n",
        "2.   The number of **movies** available on Netflix is greater than the number of **TV shows** available on Netflix."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1\n",
        "\n",
        "Average number of movies on Netflix in **United States** is greater than the average number of movies on Netflix in **India**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis: $H_o : μ_{united states} = μ_{india}$\n",
        "\n",
        "Alternate hypothesis: $H_1 : μ_{united states} \\neq μ_{india}$\n",
        "\n",
        "Test Type: Two-sample t-test"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Split the data into the 'united states' and 'india's' movie produced groups\n",
        "us_movie_df = data[data.country == 'United States']\n",
        "india_movie_df = data[data.country == 'India']"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the two-sample t-test between the release years of the two groups of movies\n",
        "import scipy\n",
        "t_stat, p_val = scipy.stats.ttest_ind(us_movie_df['release_year'], india_movie_df['release_year'], equal_var=False)\n",
        "\n",
        "# Print the results\n",
        "if p_val < 0.05:\n",
        "    print(f\"Since p-value ({p_val}) is less than 0.05, we reject null hypothesis.\\nHence, There is a significant difference in average number of movies produced by the 'United States' and 'India'.\")\n",
        "else:\n",
        "  print(f\"Since p-value ({p_val}) is greater than 0.05, we fail to reject null hypothesis.\\nHence, There is no significant difference in average number of movies produced by the 'United States' and 'India'.\")"
      ],
      "metadata": {
        "id": "d9831EzEtwte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the number of movies available on Netflix in the United States and India, I conducted a two-sample t-test, also known as an independent samples t-test or unpaired t-test. I utilized the ttest_ind function from the scipy.stats module to carry out the test, which is suitable for analyzing the means of two independent samples. By applying this test, I was able to calculate the p-value and determine if there is a significant difference in the number of movies between the two countries."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the two-sample t-test for this analysis as it is suitable for comparing the means of two independent samples. In this case, we have two separate sets of movies data from Netflix for the United States and India, and we aim to determine if there is a significant difference in the average number of movies between these two countries."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2\n",
        "The number of **movies** available on Netflix is greater than the number of **TV shows** available on Netflix."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis: $H_o : μ_{movie} = μ_{tv show}$\n",
        "\n",
        "Alternate hypothesis: $H_1 : μ_{movie} \\neq μ_{tv show}$\n",
        "\n",
        "Test Type: Two sample z-test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Count the number of movies and TV shows in the DataFrame\n",
        "n_movies = data[data['type'] == 'Movie'].count()['type']\n",
        "n_tv_shows = data[data['type'] == 'TV Show'].count()['type']"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the counts and sample sizes for the z-test\n",
        "counts = [n_movies, n_tv_shows]  # Number of movies and TV shows\n",
        "nobs = [len(data), len(data)]  # Total number of observations in the DataFrame\n",
        "\n",
        "# Perform a two sample z-test assuming equal proportions\n",
        "z_stat, p_val = proportions_ztest(counts, nobs, value=0, alternative='larger')\n",
        "\n",
        "# Print the results\n",
        "print('Z-statistic:', z_stat)\n",
        "print('P-value:', p_val)\n",
        "print()\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(f\"Since p-value ({p_val}) is less than 0.05, we reject null hypothesis.\\nHence, There is a significant difference in number of 'movies' and 'TV shows' available on Netflix.\")\n",
        "else:\n",
        "  print(f\"Since p-value ({p_val}) is greater than 0.05, we fail to reject null hypothesis.\\nHence, There is no significant difference in number of 'movies' and 'TV shows' available on Netflix.\")"
      ],
      "metadata": {
        "id": "dTe-dea5vSzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the number of movies and TV shows available on Netflix, I conducted a two-sample z-test for proportions to obtain the p-value."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose the two-sample z-test for proportions to compare the number of movies and TV shows available on Netflix because the data consists of two categorical variables."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "data.isna().sum().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "pBJu1hnWyWQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its already handled in data wrangling, so now there are no missing values to handle in the given dataset."
      ],
      "metadata": {
        "id": "ZiWk184RybDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers\n",
        "(Most of the columns are categorical, so no outliers observed)"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding\n",
        "(No need as the data is categorical)"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "# Create a new column called 'tags' in the DataFrame 'data'\n",
        "# The purpose of this column is to store text data that will be used for model building\n",
        "# The text data consists of the 'description', 'rating', 'country', 'listed_in' and 'cast' columns\n",
        "data['tags'] = data['description'] + ' ' + data['rating'] + ' ' + data['country'] + ' ' + data['listed_in'] + ' ' + data['cast']"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross checking our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "R5Qq0akFzfaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Define a function to convert text into lower cases\n",
        "def to_lower(x):\n",
        "  return x.lower()\n",
        "\n",
        "# Apply the to_lower() function to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(to_lower)\n",
        "\n",
        "# Cross checking our result for the function created\n",
        "print(data['tags'][0])\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "# Define a function to remove punctuations from text\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    # Replace each punctuation mark with no space, effectively deleting it from the text\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text_without_punct = text.translate(translator)\n",
        "    return text_without_punct\n",
        "\n",
        "# Apply the remove_punctuation function to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(remove_punctuation)\n",
        "\n",
        "# Cross-check our result that the function worked as expected\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "# 'tags' column does not have any URLs so remove words and digits containing digits\n",
        "data['tags'] = data['tags'].str.replace(r'\\w*\\d\\w*', '', regex=True)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# Since the language is english, we need to import english stop words\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def remove_stop_words(x):\n",
        "  ''' function to remove stop words'''\n",
        "  x = x.split()\n",
        "  res = ''\n",
        "  for word in x:\n",
        "    if word not in stop_words:\n",
        "      res = res + ' ' + word\n",
        "  return res\n",
        "\n",
        "# Apply the remove_stop_words function to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(remove_stop_words)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces in 'tags' column\n",
        "data['tags'] = data['tags'].str.strip()\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text\n",
        "(Not required)"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "# Rephrasing can be achived by the following code\n",
        "\n",
        "# def rephrase_tags(x):\n",
        "#     return x.replace('interesting', 'fascinating')\n",
        "# data['tags'] = data['tags'].apply(rephrase_tags)"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Loading Libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Apply the tokenization to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Cross-check our result that the function worked as expected\n",
        "print(data['tags'][0])\n",
        "\n",
        "# Store this list form of 'tags' column as 'temp_tags' for later POS tagging purpose\n",
        "temp_tags = data['tags']"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# Create an object of stemming function\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Define a function to Normalize Text function\n",
        "def stemming(text):\n",
        "    '''a function which stems each word in the given text'''\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    return \" \".join(text)\n",
        "\n",
        "# Apply the stemming function to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(stemming)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i use Stemming.\n",
        "\n",
        "Stemming is the process of reducing a word to its stem that affixes to suffixes and prefixes or to the roots of words known as \"lemmas\". Stemming is important in natural language understanding (NLU) and natural language processing (NLP). Stemming is important in natural language processing(NLP). Nil means the suffix is replaced with nothing and is just removed. There may be cases where these rules vary depending on the words. As in the case of the suffix 'ed' if the words are 'cared' and 'bumped' they will be stemmed as 'care' and 'bump'.\n",
        "\n",
        "SnowballStemmer:\n",
        "\n",
        "Snowball is a small string processing language for creating stemming algorithms for use in Information Retrieval, plus a collection of stemming algorithms implemented using it. It was originally designed and built by Martin Porter. SnowballStemmer() is a module in NLTK that implements the Snowball stemming technique."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Tagging\n",
        "# Loading Libraries\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Apply the pos tagging to the 'tags' column of the DataFrame\n",
        "data['tags'] = temp_tags.apply(nltk.pos_tag)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using this tagset we know that which tag shows which type of POS\n",
        "import nltk\n",
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "id": "5WiR4F5spVit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function which gives true word (appropriate word) after pos tagging\n",
        "def sentence(data):\n",
        "  x=\"\"\n",
        "  for i in data:\n",
        "    a=i[0]+' '\n",
        "    x=x+a\n",
        "  return x\n",
        "\n",
        "# Apply the sentence function to the 'tags' column of the DataFrame\n",
        "data['tags']=data['tags'].apply(sentence)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "cg6aTNKKp47S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the object of TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english', lowercase=False, max_features = 9000)\n",
        "\n",
        "# Fit the vectorizer using the text data\n",
        "tfidf.fit(data['tags'])\n",
        "\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dictionary = tfidf.vocabulary_.items()\n",
        "\n"
      ],
      "metadata": {
        "id": "AvcrOohM10w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert vector into array form for clustering\n",
        "vector = tfidf.transform(data['tags']).toarray()\n",
        "\n",
        "# Summarize encoded vector\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "d2MyamF4i_JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf.get_feature_names_out())"
      ],
      "metadata": {
        "id": "kHrrungFjMHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_data=pd.DataFrame(vector)\n",
        "vec_data"
      ],
      "metadata": {
        "id": "dKAAfatfjTya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have use TF-IDF techique for vectorization.\n",
        "\n",
        "TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc) in a document amongst a collection of documents (also known as a corpus).\n",
        "\n",
        "I have use TF-IDF because TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words. I can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Feature Manipulation & Selection\n",
        "(Not required)"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Transformation\n",
        "(No need to transform this data because this data is in form of Text Vectorization)"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Scaling\n",
        "(Here the units of whole data are same so no need to do scaling)"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7GRf0QnHxF3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Dimensionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes its needed, because dimensionality reduction removes the least important variables from the model. That will reduce the model's complexity and also remove some noise in the data. Its also helps to mitigate overfitting."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction (If needed)\n",
        "# Using PCA to reduce dimensionality, this might take a while..\n",
        "pca = PCA(random_state=32)\n",
        "pca.fit(vector)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G5Of3w2P4VQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Plot the cumulative explained variance\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "\n",
        "# Set labels\n",
        "plt.title('PCA - cumulative explained variance vs number of components')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.axhline(y=0.8, color='red', linestyle='--')\n",
        "plt.axvline(x=2500, color='green', linestyle='--')\n",
        "\n",
        "# Display chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ifrJW41d-W_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducing the dimensions to 2500 using pca\n",
        "pca = PCA(n_components=2500, random_state=32)\n",
        "pca.fit(vector)"
      ],
      "metadata": {
        "id": "wYDUzH7n0GJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformed features\n",
        "X = pca.transform(vector)"
      ],
      "metadata": {
        "id": "VfsBg0hQ1H8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use PCA to reduce the dimensionality of data.\n",
        "\n",
        "Because of the versatility and interpretability of PCA, it has been shown to be effective in a wide variety of contexts and disciplines. Given any high-dimensional dataset, we can start with PCA in order to visualize the relationship between points, to understand the main variance in the data, and to understand the intrinsic dimensionality.\n",
        "\n",
        "Certainly PCA is not useful for every high-dimensional dataset, but it offers a straightforward and efficient path to gaining insight into high-dimensional data."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Data Splitting\n",
        "(Not required)"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Handling Imbalanced Dataset\n",
        "(Not required)"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 : K-Means Clustering (For Metric Distortion)"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "model = KMeans()\n",
        "visualizer = KElbowVisualizer(model, k=(3,12), metric='distortion', timings=False, locate_elbow=True)\n",
        "\n",
        "# Fit the data to the visualizer\n",
        "visualizer.fit(X)\n",
        "\n",
        "# Display the plot\n",
        "visualizer.show()\n"
      ],
      "metadata": {
        "id": "qc6aQP0jtCGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this plot, we can say that the best k value is 6. Because, after this point the distortion/inertia is start decreasing in a linear fashion."
      ],
      "metadata": {
        "id": "-EC3VHLbt-EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the K-Means clustering model where number of clusters is 6\n",
        "kmean=KMeans(n_clusters=6)\n",
        "\n",
        "# Fit the data to the KMean cluster\n",
        "kmean.fit(X)\n",
        "\n",
        "# Predict on the model\n",
        "y_kmean=kmean.predict(X)"
      ],
      "metadata": {
        "id": "pB6huOWiuydQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labelling the dataset as per the cluster."
      ],
      "metadata": {
        "id": "LmB8g_4fu9nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a new column 'K_mean_cluster' in the dataset\n",
        "data[\"K_mean_cluster\"]=y_kmean\n",
        "data.head()"
      ],
      "metadata": {
        "id": "E7JXo5e2u-gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting unique labels\n",
        "u_labels = np.unique(y_kmean)\n",
        "\n",
        "# Plotting the results:\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in u_labels:\n",
        "    plt.scatter(X[y_kmean == i , 0] ,X[y_kmean == i , 1] , label = i)\n",
        "plt.title('Clusters for K-Means Clustering')\n",
        "plt.legend()\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aJBRCOgLvXvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I used K-Means Clustering.\n",
        "\n",
        "K means number of clusters.\n",
        "\n",
        "K-means is a centroid-based clustering algorithm, where we calculate the distance between each data point and a centroid to assign it to a cluster. The goal is to identify the K number of groups in the dataset.\n",
        "\n",
        "K-means clustering distinguishes itself from Hierarchical since it creates K random centroids scattered throughout the data. The algorithm looks a little bit like…\n",
        "\n",
        "(1) Initialize K random centroids.\n",
        "\n",
        "You could pick K random data points and make those your starting points.\n",
        "\n",
        "Otherwise, you pick K random values for each variable.\n",
        "\n",
        "(2) For every data point, look at which centroid is nearest to it.\n",
        "\n",
        "Using some sort of measurement like Euclidean or Cosine distance.\n",
        "\n",
        "(3) Assign the data point to the nearest centroid.\n",
        "\n",
        "(4) For every centroid, move the centroid to the average of the points assigned to that centroid.\n",
        "\n",
        "(5) Repeat the last three steps until the centroid assignment no longer changes.\n",
        "\n",
        "The algorithm is said to have “converged” once there are no more changes.\n",
        "\n",
        "These centroids act as the average representation of the points that are assigned to it. This gives you a story almost right away. You can compare the centroid values and tell if one cluster favors a group of variables or if the clusters have logical groupings of key variables."
      ],
      "metadata": {
        "id": "khnf-n9uwPsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i have use Elbow Method for optimal number of k.\n",
        "\n",
        "The elbow method is a graphical representation of finding the optimal 'K' in a K-means clustering. It works by finding WCSS (Within-Cluster Sum of Square). i.e. the sum of the square distance between points in a cluster and the cluster centroid.\n",
        "\n",
        "In cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters and picking the elbow of the curve as the number of clusters to use."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can not directly predict the number of cluster. After using Elbow method we can get optimal number of clusters and we can implement it directly."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 : Hierarchical Clustering (Agglomerative Clustering)"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "# Using the dendrogram to find the optimal number of clusters\n",
        "\n",
        "# Instantiate the dendogram\n",
        "plt.figure(figsize=(13,6))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "\n",
        "# Set labels\n",
        "plt.title('Dendrogram')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.axhline(y=5.8, color='r', linestyle='--')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qiWjmDUTyCnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this graph we can say that optimal number of clusters is around 6."
      ],
      "metadata": {
        "id": "dRFIAFAS0PLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the Agglomerative clustering model where number of clusters is 6\n",
        "aggh = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')\n",
        "\n",
        "# Fit the data to the Agglomerative cluster\n",
        "aggh.fit(X)\n",
        "\n",
        "# Predict on the model\n",
        "y_hc=aggh.fit_predict(X)"
      ],
      "metadata": {
        "id": "1v6vIyEx0ZrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a new column 'Agg_cluster' in the dataset\n",
        "data[\"Agg_cluster\"]=y_hc\n",
        "data.head()"
      ],
      "metadata": {
        "id": "rWnT3a9Q0fe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting unique labels\n",
        "u_labels = np.unique(y_hc)\n",
        "\n",
        "# Plotting the results:\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in u_labels:\n",
        "    plt.scatter(X[y_hc == i , 0] ,X[y_hc == i , 1] , label = i)\n",
        "plt.title('Clusters for Agglomerative Clustering')\n",
        "plt.legend()\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MNdF7OpC0qYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i used Agglomerative Clustering.\n",
        "\n",
        "Agglomerative Clustering is a type of hierarchical clustering algorithm. It is an unsupervised machine learning technique that divides the population into several clusters such that data points in the same cluster are more similar and data points in different clusters are dissimilar.\n",
        "\n",
        "Agglomerative Hierarchical Clustering (AHC) is an iterative classification method whose principle is simple.\n",
        "\n",
        "(1) The process starts by calculating the dissimilarity between the N objects.\n",
        "\n",
        "(2) Then two objects which when clustered together minimize a given agglomeration criterion, are clustered together thus creating a class comprising these two objects.\n",
        "\n",
        "(3) Then the dissimilarity between this class and the N-2 other objects is calculated using the agglomeration criterion. The two objects or classes of objects whose clustering together minimizes the agglomeration criterion are then clustered together.\n",
        "\n",
        "This process continues until all the objects have been clustered."
      ],
      "metadata": {
        "id": "8yaVmBlo1ASG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have use Dendrogram for optimal number of clusters.\n",
        "\n",
        "A dendrogram is a branching diagram that represents the relationships of similarity among a group of entities. Each branch is called a clade. There is no limit to the number of leaves in a clade.\n",
        "\n",
        "A dendrogram is a diagram that shows the attribute distances between each pair of sequentially merged classes. To avoid crossing lines, the diagram is graphically arranged so that members of each pair of classes to be merged are neighbors in the diagram. The Dendrogram tool uses a hierarchical clustering algorithm.\n",
        "\n",
        "A dendrogram is a tree-structured graph used in heat maps to visualize the result of a hierarchical clustering calculation. The result of a clustering is presented either as the distance or the similarity between the clustered rows or columns depending on the selected distance measure."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we get optimal number of cluster is 6.\n",
        "\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we cannot directly predict the number of cluster . After plotting dendrogram chart we can get optimal number of clusters and we can implement it directly in the data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Finding optimal number of clusters using the Silhouette Score\n",
        "for n_clusters in range(2,15):\n",
        "  km = KMeans (n_clusters=n_clusters, init ='k-means++', random_state=51)\n",
        "  km.fit(X)\n",
        "  preds = km.predict(X)\n",
        "  centers = km.cluster_centers_\n",
        "  score = silhouette_score(X, preds, metric='euclidean')\n",
        "  print (\"For n_clusters = %d, silhouette score is %0.4f\"%(n_clusters, score))"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this chart we can say that optimal number of cluster is 5. Because the silhouette score is highest for the cluster 5."
      ],
      "metadata": {
        "id": "Ehbvz4eN9vZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Silhouette Plots for Each Clusters\n",
        "# Instantiate the clustering model and visualizer\n",
        "for n_clusters in range(2,15):\n",
        "    km = KMeans (n_clusters=n_clusters, init ='k-means++', random_state=51)\n",
        "    km.fit(X)\n",
        "    preds = km.predict(X)\n",
        "    centers = km.cluster_centers_\n",
        "\n",
        "    # Set parameters and labels\n",
        "    score = silhouette_score(X, preds, metric='euclidean')\n",
        "    print (\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))\n",
        "\n",
        "    visualizer = SilhouetteVisualizer(km)\n",
        "\n",
        "    visualizer.fit(X) # Fit the training data to the visualizer\n",
        "    visualizer.poof() # Draw/show/poof the data"
      ],
      "metadata": {
        "id": "hRfPmv2V7FqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of cluster is defined by Silhouette Coefficient.\n",
        "\n",
        "Silhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1.\n",
        "\n",
        "1: Means clusters are well apart from each other and clearly distinguished.\n",
        "\n",
        "0: Means clusters are indifferent, or we can say that the distance between clusters is not significant.\n",
        "\n",
        "-1: Means clusters are assigned in the wrong way.\n",
        "\n",
        "The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a, b). To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of."
      ],
      "metadata": {
        "id": "L46Idb1S7liS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "itelEseC8NjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "poZOCxLs8b6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we got Silhouette Coefficient for optimal number of clusters. From this data we got optimal number of clusters is 5 because it has a higher Silhouette Coefficient."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we cannot directly predict the number of cluster . After using this method we can get optimal number of clusters and we can implement it directly in data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette score is the best evaluation metric for optimization the number of clusters.\n",
        "\n",
        "The optimal number of cluster gives us the lightness and transparency of the business.\n",
        "\n",
        "Through cluster we can find out which type of customers are in our data.\n",
        "\n",
        "This evaluation metric makes business decision easier. Getting the Silhouette score is very easy."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the charts we can see that K-Mean Clustering model is best model for our data.\n",
        "\n",
        "Here we get optimal number of clusters is 6, but often the number of clusters is already determined within the business. If the number of clusters within a business is already determined, we can apply the algorithm well.\n",
        "\n",
        "Within the K-Mean Cluster graph we can see that the clusters are well divided.\n",
        "\n",
        "Through this cluster we can know what type of data is in which cluster.\n",
        "\n",
        "The goal of this problems may be to discover groups of similar examples within the data.\n",
        "\n",
        "The primary function of this algorithm is to perform segmentation, whether it is store, product, or customer. Customers and products can be clustered into hierarchical groups based on different attributes."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count Plot Visualization Code for number of movies and tv shows in each cluster\n",
        "# Set labels\n",
        "plt.figure(figsize=(12,6))\n",
        "graph = sns.countplot(x='K_mean_cluster',data=data, hue='type', palette=['#564d4d', '#db0000'])\n",
        "plt.title('Number of Movies and TV shows in each cluster - K-Means Clustering')\n",
        "plt.xlabel('Kmeans Clusters')\n",
        "\n",
        "# Adding value count on the top of bar\n",
        "for p in graph.patches:\n",
        "   graph.annotate(format(p.get_height(), '.0f'), (p.get_x(), p.get_height()), xytext = (0,3), textcoords = 'offset points')"
      ],
      "metadata": {
        "id": "lk9jkCMPBvVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do Cluster Analysis....................\n",
        "*  Clustering similar content by matching text-based features"
      ],
      "metadata": {
        "id": "-oPueOI39rDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud Plot Visualization Code for User Rating Review\n",
        "# Define a Function for Clustering Similar Content by Matching Text-Based Features\n",
        "def kmeans_worldcloud(cluster_num):\n",
        "\n",
        " # Create a String to Store All The Words\n",
        "  comment_words = ''\n",
        "\n",
        "  # Remove The Stopwords\n",
        "  stopwords = set(STOPWORDS)\n",
        "\n",
        "  # Iterate Through The Column\n",
        "  for val in data[data['K_mean_cluster']==cluster_num].tags.values:\n",
        "\n",
        "      # Typecaste Each Val to String\n",
        "      val = str(val)\n",
        "\n",
        "      # Split The Value\n",
        "      tokens = val.split()\n",
        "\n",
        "      # Converts Each Token into lowercase\n",
        "      for i in range(len(tokens)):\n",
        "          tokens[i] = tokens[i].lower()\n",
        "\n",
        "      comment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "  # Set Parameters\n",
        "  wordcloud = WordCloud(width = 1000, height = 500,\n",
        "                  background_color ='white',\n",
        "                  stopwords = stopwords,\n",
        "                  min_font_size = 10,\n",
        "                  max_words = 1000,\n",
        "                  colormap = 'gist_heat_r').generate(comment_words)\n",
        "\n",
        "  # Set Labels\n",
        "  plt.figure(figsize = (6,6), facecolor = None)\n",
        "  plt.title(f'Most Important Words In Cluster {cluster_num}', fontsize = 15, pad=20)\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad = 0)\n",
        "\n",
        "  # Display Chart\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "THvcO07O9e5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for cluster 0\n",
        "kmeans_worldcloud(0)\n"
      ],
      "metadata": {
        "id": "aQBrCtY7-cjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for cluster 1\n",
        "kmeans_worldcloud(1)\n"
      ],
      "metadata": {
        "id": "mSr9Eo9V_tOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for cluster 2\n",
        "kmeans_worldcloud(2)"
      ],
      "metadata": {
        "id": "C0yuDk1LCl3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i have use topic modeling. Assume that the clusters are topics. Here for topic modeling i use CountVectorizer process for Vectorization of data and i use Latent Dirichlet Allocation for building a topic."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use count vectorization process for our data\n",
        "# Create a count vectorizer object\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the count vectorizer using the text data\n",
        "document_term_matrix=count_vectorizer.fit_transform(data['tags'])"
      ],
      "metadata": {
        "id": "r4MMNwZ_BRkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA model\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components=6)\n",
        "lda.fit_transform(document_term_matrix)\n"
      ],
      "metadata": {
        "id": "BVN6t6s9Bl8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Most Important Features for Each Topic\n",
        "vocab = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "for i, comp in enumerate(lda.components_):\n",
        "    vocab_comp = zip(vocab, comp)\n",
        "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:5]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    for t in sorted_words:\n",
        "        print(t[0],end=\" \")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "tgR6sS9he8VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate WordCloud Images for Given Topics\n",
        "# Define a Function for Visualize Most Important Features for Each Topic\n",
        "def draw_word_cloud(topic_num):\n",
        "\n",
        "  # Create a String to Store All The Words\n",
        "  imp_words_topic=\"\"\n",
        "\n",
        "  # Set Parameters\n",
        "  comp=lda.components_[topic_num]\n",
        "  vocab_comp = zip(vocab, comp)\n",
        "  sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:25]\n",
        "  for word in sorted_words:\n",
        "    imp_words_topic=imp_words_topic+\" \"+word[0]\n",
        "\n",
        "  # Set Parameters\n",
        "  wordcloud = WordCloud(width = 1000, height = 500,\n",
        "                  background_color ='white',\n",
        "                  stopwords = stopwords,\n",
        "                  min_font_size = 10,\n",
        "                  max_words = 1000,\n",
        "                  colormap = 'gist_heat_r').generate(imp_words_topic)\n",
        "\n",
        "  # Set Labels\n",
        "  plt.figure(figsize = (6,6), facecolor = None)\n",
        "  plt.title(f'Most Important Features in Topic {topic_num}', fontsize = 15, pad=20)\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad = 0)\n",
        "\n",
        "  #Display Chart\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "RTwTw7GefLGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for Topic 2\n",
        "draw_word_cloud(2)"
      ],
      "metadata": {
        "id": "Fiy9voK3fd1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for Topic 3\n",
        "draw_word_cloud(3)"
      ],
      "metadata": {
        "id": "oRZsYzopfm5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for Topic 4\n",
        "draw_word_cloud(4)"
      ],
      "metadata": {
        "id": "rlrXKy4Kf4q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for Topic 5\n",
        "draw_word_cloud(5)"
      ],
      "metadata": {
        "id": "ThB73BVCgAW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here from those wordcloud plots we can know which word is important for which cluster. We can also decide the name of Topics (Clusters) from this plots."
      ],
      "metadata": {
        "id": "aobj9z9LgTfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8. Content Based Recommender System***"
      ],
      "metadata": {
        "id": "CZM08lAOLDwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a new dataframe for building a recommender system\n",
        "recommender_df = data.copy()\n",
        "\n",
        "# Resetting index\n",
        "recommender_df.reset_index(inplace=True)\n",
        "\n",
        "# Dropping show-id and index column\n",
        "recommender_df = recommender_df.drop(columns=['index', 'show_id'])\n",
        "\n"
      ],
      "metadata": {
        "id": "Op5JdMrjjbTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity on transformed array independent features created from tags(cluster) column\n",
        "similarity = cosine_similarity(X)"
      ],
      "metadata": {
        "id": "IpTQKgV3P638"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity on transformed array independent features created from tags(cluster) column\n",
        "similarity = cosine_similarity(X)"
      ],
      "metadata": {
        "id": "FrsxERGgLd5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function which recommends top 10 shows on the basis of cosine similarity score\n",
        "def recommend(content):\n",
        "  try:\n",
        "    '''\n",
        "    This function recommends top 10 shows similar to the input show based on their similarity scores.\n",
        "    '''\n",
        "\n",
        "    # Find the index position of the input content\n",
        "    index = recommender_df[recommender_df['title'] == content].index[0]\n",
        "\n",
        "    # Sorting on the basis of similarity score, in order to find out distances from recommended one\n",
        "    distances = sorted(list(enumerate(similarity[index])), reverse=True, key=lambda x:x[1])\n",
        "\n",
        "    # Display the input movie/tv show name\n",
        "    print('--'*30)\n",
        "    print(f\"Since you liked '{content}', you may also like:\")\n",
        "    print('--'*30)\n",
        "\n",
        "    # List the top ten recommended movies/tv shows\n",
        "    for i in distances[1:11]:\n",
        "      print(data.iloc[i[0]].title)\n",
        "  except:\n",
        "     print(f\"Didn't find any matches for '{content}'. Browse other popular TV shows and movies.\")"
      ],
      "metadata": {
        "id": "BmUuBPR0LvHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Recommender System on a Indian Movie\n",
        "recommend('Zindagi Na Milegi Dobara')"
      ],
      "metadata": {
        "id": "b8qiEy2kLxfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Recommender System on a International Movie\n",
        "recommend('Avengers: Infinity War')"
      ],
      "metadata": {
        "id": "IiGq04UUL9Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Recommender System on a Korean TV Show\n",
        "recommend('What in the World Happened?')"
      ],
      "metadata": {
        "id": "nlJqEeQTjA5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Recommender System on a Content, Which is Not Listed in Netflix Dataset\n",
        "recommend('Avenger')"
      ],
      "metadata": {
        "id": "f88QRJXQjMgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***9.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "\n",
        "# Serialize process (wb=write byte)\n",
        "# Save the best model (KMeans Clustering)\n",
        "pickle.dump(kmean,open('kmeans_model.pkl','wb'))"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "# Unserialize process (rb=read byte)\n",
        "pickled_model= pickle.load(open('kmeans_model.pkl','rb'))\n",
        "\n",
        "# Predicting the unseen data\n",
        "pickled_model.predict(X)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_kmean"
      ],
      "metadata": {
        "id": "kUT-5gJtQ9mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of the project was to cluster TV shows and movies based on their similarities and differences, with the ultimate goal of creating a content-based recommender system that recommends 10 shows to users based on their viewing history. Some key points from the project include:\n",
        "\n",
        "* Exploring the dataset consist of 7787 records and 12 attributes, with a focus on missing value imputation and exploratory data analysis (EDA).\n",
        "\n",
        "* The analysis revealed that Netflix has a greater number of movies than TV shows, with a rapidly growing collection of shows from the United States.\n",
        "\n",
        "* To cluster the shows, i have selected six key attributes: director, cast, country, genre, rating, and description (all are categorical variables). These attributes were transformed into a 9000-feature TF-IDF vectorization, and Principal Component Analysis (PCA) was used to address the curse of dimensionality. Captured more than 80% of the variance by reducing the components to 2500.\n",
        "\n",
        "* Next, i used K-Means and Agglomerative clustering algorithms to group the shows. The elbow method confirmed that the optimal number of clusters was 6 for K-Means, however for Silhouette score analysis it was 5.\n",
        "\n",
        "* In Agglomerative clustering the optimal number of clusters was also 6, which we visualized with a dendrogram.\n",
        "\n",
        "* Continued all the efforts by creating a content-based recommender system using the similarity matrix obtained through cosine similarity.\n",
        "\n",
        "The recommender system offers personalized recommendations based on the type of shows the user has watched and provides the user with ten top-notch suggestions to explore."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}